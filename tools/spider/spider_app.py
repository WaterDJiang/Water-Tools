# spider_app.py
# çˆ¬è™«å·¥å…·ä¸»å…¥å£
# ä½œè€…ï¼šWater.D.J
# è¯´æ˜ï¼šæœ¬æ–‡ä»¶ä¸ºçˆ¬è™«å·¥å…·çš„ä¸»å…¥å£ï¼ŒåŒ…å«å…ƒä¿¡æ¯å’Œä¸»ç•Œé¢é€»è¾‘ã€‚

import streamlit as st
import pandas as pd
import io
import re
from bs4 import BeautifulSoup, Comment
from tools.spider.batch_scraper import batch_scraper_main
from tools.spider.wechat_links import wechat_links_main
from playwright.sync_api import sync_playwright
from tools.spider.common import clean_content, show_results, show_scrollable_preview

# é¡¹ç›®å…ƒä¿¡æ¯ï¼Œä¾›ä¸»å…¥å£è‡ªåŠ¨èšåˆ
PROJECT_META = {
    "name": "é€šç”¨çˆ¬è™«å·¥å…·",
    "key": "spider",
    "desc": "æ”¯æŒè‡ªå®šä¹‰ç›®æ ‡ç½‘ç«™ã€æ‰¹é‡ä¸Šä¼ é“¾æ¥ã€å…¬ä¼—å·ä¸“è¾‘æ‰¹é‡é‡‡é›†çš„é€šç”¨çˆ¬è™«å·¥å…·ã€‚",
    "entry": "tools/spider/spider_app.py"
}

# å·¥å…·å‡½æ•°ï¼šæ¸…ç† HTML å†…å®¹ä¸ºçº¯æ–‡æœ¬
def clean_content(raw_html):
    """
    è¿›ä¸€æ­¥æ¸…ç†ï¼šå»é™¤script/style/æ³¨é‡Šï¼Œå…¼å®¹å¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ã€CSDNç­‰å¹³å°ï¼Œä¼˜å…ˆæå–ä¸»æµæ­£æ–‡å®¹å™¨ã€‚
    """
    if not isinstance(raw_html, str):
        return ""
    soup = BeautifulSoup(raw_html, "html.parser")
    # å»é™¤æ‰€æœ‰è„šæœ¬å’Œæ ·å¼
    for tag in soup(["script", "style"]):
        tag.decompose()
    # å»é™¤æ³¨é‡Š
    for element in soup(text=lambda text: isinstance(text, Comment)):
        element.extract()
    # æå–æ ‡é¢˜
    title = soup.title.string.strip() if soup.title else ""
    # æå–metaæè¿°
    meta_desc = ""
    meta = soup.find("meta", attrs={"name": "description"})
    if meta and meta.get("content"):
        meta_desc = meta["content"].strip()
    # å¹³å°æ­£æ–‡æå–
    main_content = ""
    # å¾®ä¿¡å…¬ä¼—å·
    js_content = soup.find("div", id="js_content")
    if js_content:
        # åªæå–js_contentä¸‹æ‰€æœ‰<p>æ ‡ç­¾æ–‡æœ¬
        main_content = " ".join([p.get_text(strip=True) for p in js_content.find_all("p")])
    else:
        # çŸ¥ä¹ï¼ˆæ–°ç‰ˆ/è€ç‰ˆï¼‰
        zhihu_main = soup.find("div", class_="RichText ztext") or \
                     soup.find("div", class_="RichText") or \
                     soup.find("div", class_="Post-RichTextContainer")
        if zhihu_main:
            main_content = zhihu_main.get_text(separator=' ', strip=True)
        else:
            # CSDN
            csdn_main = soup.find("div", class_="article_content") or \
                        soup.find("div", id="content_views") or \
                        soup.find("article", class_="baidu_pl") or \
                        soup.find("div", class_="blog-content-box") or \
                        soup.find("main")
            if csdn_main:
                main_content = csdn_main.get_text(separator=' ', strip=True)
            else:
                # é€€è€Œæ±‚å…¶æ¬¡ï¼Œæå–æ‰€æœ‰<p>æ ‡ç­¾æ–‡æœ¬
                main_content = " ".join([p.get_text(strip=True) for p in soup.find_all("p")])
    # åˆå¹¶
    result = f"æ ‡é¢˜ï¼š{title}\næè¿°ï¼š{meta_desc}\næ­£æ–‡ï¼š{main_content}"
    # å†å»é™¤å¤šä½™ç©ºç™½
    result = re.sub(r'\s+', ' ', result)
    return result.strip()

def single_crawl_tab():
    with st.container():
        url = st.text_input("è¯·è¾“å…¥è¦çˆ¬å–çš„ç½‘é¡µé“¾æ¥ï¼š", placeholder="å¦‚ï¼šhttps://mp.weixin.qq.com/s/xxx")
        col1, col2 = st.columns([2,1])
        with col1:
            if st.button("å¼€å§‹çˆ¬å–", key="single_crawl_btn"):
                if url:
                    st.info(f"å³å°†çˆ¬å–ï¼š{url}")
                    with st.spinner("æ­£åœ¨æœ¬åœ°çˆ¬å–å¹¶è§£æå†…å®¹ï¼Œè¯·ç¨å€™..."):
                        try:
                            with sync_playwright() as p:
                                browser = p.chromium.launch(headless=True)
                                page = browser.new_page(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                                page.goto(url, wait_until='domcontentloaded', timeout=60000)
                                page.wait_for_timeout(3000)
                                content = page.content()
                                cleaned = clean_content(content)
                                browser.close()
                            df = pd.DataFrame([{"url": url, "content": cleaned}])
                            st.session_state['single_crawl_result'] = df
                        except Exception as e:
                            st.error(f"æœ¬åœ°çˆ¬å–å¼‚å¸¸ï¼š{e}")
                else:
                    st.warning("è¯·å…ˆè¾“å…¥æœ‰æ•ˆé“¾æ¥ï¼")
        # ç»“æœåŒº
        if 'single_crawl_result' in st.session_state:
            df = st.session_state['single_crawl_result']
            st.success("å·²åŠ è½½ä¸Šæ¬¡å•é“¾æ¥çˆ¬å–ç»“æœã€‚")
            st.dataframe(df)
            st.markdown("**å†…å®¹é¢„è§ˆï¼š**")
            show_scrollable_preview(df.iloc[0]["content"] if pd.notnull(df.iloc[0]["content"]) else "(æ— å†…å®¹)")
            csv_buf = io.StringIO()
            df.to_csv(csv_buf, index=False, encoding="utf-8-sig")
            st.download_button(
                label="ä¸‹è½½ CSV æ–‡ä»¶",
                data=csv_buf.getvalue(),
                file_name="crawl_result.csv",
                mime="text/csv"
            )

def main():
    tab_desc = [

        "å•ç½‘é¡µå†…å®¹é‡‡é›†",
        "æ‰¹é‡é“¾æ¥å†…å®¹é‡‡é›†",
        "å…¬ä¼—å·ä¸“è¾‘æ‰¹é‡é‡‡é›†",
        "ğŸ“– ä½¿ç”¨è¯´æ˜"
    ]
    tab1, tab2, tab3,tab0  = st.tabs(tab_desc)
    with tab0:
        st.title("é€šç”¨çˆ¬è™«å·¥å…· - ä½¿ç”¨è¯´æ˜")
        st.markdown("""
### å·¥å…·ç®€ä»‹
æœ¬å·¥å…·é›†æˆäº†ä¸‰å¤§çˆ¬è™«åŠŸèƒ½ï¼Œé€‚ç”¨äºä¸åŒç½‘é¡µå†…å®¹é‡‡é›†åœºæ™¯ï¼š

1. **å•ç½‘é¡µå†…å®¹é‡‡é›†**ï¼šè¾“å…¥ä»»æ„ç½‘é¡µé“¾æ¥ï¼Œè‡ªåŠ¨é‡‡é›†æ­£æ–‡å†…å®¹ï¼Œæ”¯æŒä¸»æµå¹³å°ï¼ˆå¦‚å¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ã€CSDNç­‰ï¼‰ï¼Œå†…å®¹å¯é¢„è§ˆå’Œä¸‹è½½ã€‚
2. **æ‰¹é‡é“¾æ¥å†…å®¹é‡‡é›†**ï¼šä¸Šä¼ åŒ…å«å¤šä¸ªé“¾æ¥çš„CSVæ–‡ä»¶ï¼Œè‡ªåŠ¨å¹¶å‘é‡‡é›†æ‰€æœ‰ç½‘é¡µå†…å®¹ï¼Œæ”¯æŒè‡ªå®šä¹‰å¹¶å‘æ•°ï¼Œç»“æœå¯é¢„è§ˆå’Œæ‰¹é‡ä¸‹è½½ã€‚
3. **å…¬ä¼—å·ä¸“è¾‘æ‰¹é‡é‡‡é›†**ï¼šè¾“å…¥å…¬ä¼—å·ä¸“è¾‘é¡µåœ°å€ï¼Œè‡ªåŠ¨è¯†åˆ«æ‰€æœ‰æ–‡ç« é“¾æ¥å¹¶å¯æ‰¹é‡é‡‡é›†å†…å®¹ï¼Œæ”¯æŒè¿›åº¦æ˜¾ç¤ºå’Œç»“æœä¸‹è½½ã€‚

### ä½¿ç”¨æµç¨‹
1. é€‰æ‹©å¯¹åº”åŠŸèƒ½é¡µç­¾ã€‚
2. æŒ‰é¡µé¢æç¤ºè¾“å…¥é“¾æ¥æˆ–ä¸Šä¼ æ–‡ä»¶ã€‚
3. è®¾ç½®å‚æ•°ï¼ˆå¦‚å¹¶å‘æ•°ã€é‡‡é›†æ•°é‡ç­‰ï¼‰ã€‚
4. ç‚¹å‡»æ“ä½œæŒ‰é’®ï¼Œç­‰å¾…è¿›åº¦æ¡å®Œæˆã€‚
5. åœ¨ç»“æœåŒºé¢„è§ˆå†…å®¹å¹¶ä¸‹è½½CSVã€‚

### æ³¨æ„äº‹é¡¹
- å»ºè®®ç§‘å­¦ä¸Šç½‘ï¼Œéƒ¨åˆ†ç½‘é¡µéœ€ç™»å½•æˆ–æœ‰åçˆ¬æœºåˆ¶ï¼Œé‡‡é›†ç»“æœå¯èƒ½å—é™ã€‚
- å¹¶å‘æ•°å»ºè®®æ ¹æ®æœ¬æœºæ€§èƒ½å’Œç½‘ç»œçŠ¶å†µè°ƒæ•´ï¼Œè¿‡é«˜å¯èƒ½å¯¼è‡´å¤±è´¥ã€‚
- å…¬ä¼—å·ä¸“è¾‘é‡‡é›†ä»…æ”¯æŒå…¬å¼€ä¸“è¾‘é¡µï¼Œéƒ¨åˆ†å†…å®¹å¦‚æœ‰å¼‚å¸¸è¯·åé¦ˆã€‚
- æ‰€æœ‰é‡‡é›†ç»“æœä¼šè‡ªåŠ¨ä¿å­˜åœ¨é¡µé¢ä¼šè¯ï¼ˆsession_stateï¼‰ä¸­ï¼Œé¡µé¢ä¸åˆ·æ–°å¯å¤šæ¬¡ä¸‹è½½ã€‚
        """)
    with tab1:
        single_crawl_tab()
    with tab2:
        batch_scraper_main()
    with tab3:
        wechat_links_main()
